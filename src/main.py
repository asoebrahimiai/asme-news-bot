import os
import requests
import xml.etree.ElementTree as ET
from bs4 import BeautifulSoup
from datetime import datetime, timezone
import time
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query

# â”€â”€â”€ ØªÙ†Ø¸ÛŒÙ…Ø§Øª â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN    = os.environ.get("TELEGRAM_TOKEN", "")
TELEGRAM_CHAT     = os.environ.get("TELEGRAM_CHAT", "")
APPWRITE_ENDPOINT = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT  = os.environ.get("APPWRITE_PROJECT", "")
APPWRITE_KEY      = os.environ.get("APPWRITE_KEY", "")
DATABASE_ID       = os.environ.get("DATABASE_ID", "")
COLLECTION_ID     = os.environ.get("COLLECTION_ID", "")

MAX_NEWS = 10

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
}

# Ù…Ù†Ø§Ø¨Ø¹ RSS - Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª
RSS_SOURCES = [
    # ÙÛŒØ¯ Ø±Ø³Ù…ÛŒ ASME News
    "https://www.asme.org/rss/news",
    # ÙÛŒØ¯ Topics & Resources
    "https://www.asme.org/rss/topics-resources",
]

# Ú©Ù„ÛŒØ¯ÙˆØ§Ú˜Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø¨Ø±Ø§ÛŒ Ø§Ø®Ø¨Ø§Ø± Ù…Ø±ØªØ¨Ø· Ø¨Ø§ ASME
ASME_KEYWORDS = [
    "asme", "mechanical engineer", "engineering", "fellow",
    "award", "standard", "manufacturing", "robotics", "aerospace"
]


# â”€â”€â”€ Ø®ÙˆØ§Ù†Ø¯Ù† RSS ÙÛŒØ¯ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_rss_news() -> list:
    """Ø§Ø®Ø¨Ø§Ø± Ø±Ø§ Ø§Ø² RSS ÙÛŒØ¯Ù‡Ø§ÛŒ ASME Ø¯Ø±ÛŒØ§ÙØª Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""
    news_items = []
    seen_urls = set()

    for rss_url in RSS_SOURCES:
        try:
            print(f"ğŸ“¡ Ø¯Ø±ÛŒØ§ÙØª RSS: {rss_url}")
            resp = requests.get(rss_url, headers=HEADERS, timeout=15)
            resp.raise_for_status()

            root = ET.fromstring(resp.content)
            channel = root.find("channel")
            if channel is None:
                continue

            for item in channel.findall("item"):
                title = item.findtext("title", "").strip()
                link  = item.findtext("link",  "").strip()
                desc  = item.findtext("description", "").strip()

                if not title or not link:
                    continue
                if link in seen_urls:
                    continue
                if len(title) < 15:
                    continue

                seen_urls.add(link)
                news_items.append({
                    "title": title,
                    "url": link,
                    "description": desc
                })

                if len(news_items) >= MAX_NEWS:
                    break

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± RSS {rss_url}: {e}")
            continue

        if len(news_items) >= MAX_NEWS:
            break

    # Ø§Ú¯Ù‡ RSS Ú©Ø§Ø± Ù†Ú©Ø±Ø¯ØŒ Ø§Ø² scrape Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    if not news_items:
        print("ğŸ”„ RSS Ù†Ø§Ù…ÙˆÙÙ‚ Ø¨ÙˆØ¯ØŒ ØªÙ„Ø§Ø´ Ø¨Ø§ scrape...")
        news_items = scrape_asme_news_page()

    print(f"âœ… {len(news_items)} Ø®Ø¨Ø± ÛŒØ§ÙØª Ø´Ø¯")
    return news_items[:MAX_NEWS]


# â”€â”€â”€ Ø§Ø³Ú©Ø±Ù¾ Ù…Ø³ØªÙ‚ÛŒÙ… ØµÙØ­Ù‡ ASME News (Fallback) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_asme_news_page() -> list:
    """
    Fallback: Ø§Ø³Ú©Ø±Ù¾ Ù…Ø³ØªÙ‚ÛŒÙ… Ø§Ø² ØµÙØ­Ù‡ Ø§Ø®Ø¨Ø§Ø± ASME.
    Ø§ÛŒÙ† ØµÙØ­Ù‡ Ù†Ø³Ø¨ØªØ§Ù‹ Ø§ÛŒØ³ØªØ§Ø³Øª.
    """
    urls_to_try = [
        "https://www.asme.org/topics-resources/society-news",
        "https://www.asme.org/about-asme/news",
    ]

    for page_url in urls_to_try:
        try:
            resp = requests.get(page_url, headers=HEADERS, timeout=20)
            resp.raise_for_status()
            soup = BeautifulSoup(resp.content, "html.parser")

            news_items = []
            seen_urls = set()

            # Ø¬Ø³ØªØ¬Ùˆ Ø¯Ø± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØµÙØ­Ù‡
            for a in soup.find_all("a", href=True):
                href = a["href"].strip()
                text = a.get_text(strip=True)

                # ÙÙ‚Ø· Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ ASME
                if href.startswith("/"):
                    href = "https://www.asme.org" + href
                elif not href.startswith("https://www.asme.org"):
                    continue

                # ÙÛŒÙ„ØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù†ÙˆÛŒ Ù†Ø§ÙˆØ¨Ø±ÛŒ
                skip_paths = [
                    "/cart", "/search", "/sign-in", "/membership",
                    "/codes-standards", "/about-asme/media",
                    "/about-asme/contact", "/about-asme/careers",
                    "/learning-development", "/conferences-events",
                    "/get-involved", "/sitemap", "/terms", "/privacy",
                ]
                if any(sp in href for sp in skip_paths):
                    continue

                if len(text) < 20 or href in seen_urls:
                    continue

                # ÙÙ‚Ø· Ù…Ù‚Ø§Ù„Ø§Øª Ø®Ø¨Ø±ÛŒ
                if "/topics-resources/" in href or "/society-news/" in href or "/news/" in href:
                    seen_urls.add(href)
                    news_items.append({"title": text, "url": href, "description": ""})

                if len(news_items) >= MAX_NEWS:
                    break

            if news_items:
                print(f"âœ… {len(news_items)} Ø®Ø¨Ø± Ø§Ø² {page_url} ÛŒØ§ÙØª Ø´Ø¯")
                return news_items

        except Exception as e:
            print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± scrape {page_url}: {e}")

    return []


# â”€â”€â”€ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_article_text(url: str, fallback_desc: str = "") -> str:
    """Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯."""

    # Ø§Ú¯Ù‡ URL Ø§Ø² NewsBreak ÛŒØ§ Ø³Ø§ÛŒØªâ€ŒÙ‡Ø§ÛŒ JS-heavy Ø§Ø³ØªØŒ Ø§Ø² description Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
    js_heavy_domains = ["newsbreak.com", "medium.com", "substack.com"]
    if any(d in url for d in js_heavy_domains):
        print(f"   âš ï¸ Ø³Ø§ÛŒØª JS-heavyØŒ Ø§Ø² description Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯")
        return fallback_desc[:400] if fallback_desc else ""

    try:
        resp = requests.get(url, headers=HEADERS, timeout=12)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "html.parser")

        # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
        for tag in soup(["script", "style", "nav", "footer", "header",
                         "aside", "form", "iframe", "noscript",
                         "figure", "figcaption", "picture"]):
            tag.decompose()

        # Ø­Ø°Ù Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
        for tag in soup.find_all(True):
            cls = " ".join(tag.get("class", []))
            if any(w in cls.lower() for w in [
                "sidebar", "related", "recommend", "comment",
                "social", "share", "promo", "newsletter",
                "subscribe", "popup", "ad-", "-ad"
            ]):
                tag.decompose()

        # Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ Ø¨Ù‡ ØªØ±ØªÛŒØ¨ Ø§ÙˆÙ„ÙˆÛŒØª
        selectors = [
            "article .content",
            "article",
            '[class*="article-body"]',
            '[class*="article-content"]',
            '[class*="story-body"]',
            '[class*="post-content"]',
            '[class*="entry-content"]',
            "main",
            ".content",
        ]

        # Ú©Ù„Ù…Ø§Øª ÙÛŒÙ„ØªØ± Ù†Ø§Ù…Ø±Ø¨ÙˆØ· (Ù…Ø°Ù‡Ø¨ÛŒØŒ ØªØ§Ø±ÛŒØ®ÛŒØŒ Ùˆ ØºÛŒØ±Ù‡)
        skip_phrases = [
            "cookie", "subscribe", "newsletter", "advertisement",
            "sign up", "log in", "privacy policy", "terms of use",
            "copyright Â©", "all rights reserved",
            # Ù…ØªÙˆÙ† Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
            "jordan river", "gilead", "ephraim", "shibboleth",
            "passover", "biblical", "scripture", "testament",
        ]

        for selector in selectors:
            el = soup.select_one(selector)
            if not el:
                continue

            paragraphs = el.find_all("p")
            text_parts = []

            for p in paragraphs:
                t = p.get_text(separator=" ", strip=True)
                if len(t) < 80:
                    continue
                if any(ph in t.lower() for ph in skip_phrases):
                    continue
                text_parts.append(t)
                if len(text_parts) >= 2:
                    break

            if text_parts:
                combined = " ".join(text_parts)
                return combined[:500]

        # Fallback: description Ø§Ø² RSS
        return fallback_desc[:400] if fallback_desc else ""

    except Exception as e:
        print(f"   âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬: {e}")
        return fallback_desc[:400] if fallback_desc else ""


# â”€â”€â”€ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ MyMemory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_to_fa(text: str) -> str:
    if not text or len(text.strip()) < 5:
        return ""

    text = text[:480]

    try:
        resp = requests.get(
            "https://api.mymemory.translated.net/get",
            params={
                "q": text,
                "langpair": "en|fa",
                "de": "asmenewsbot@gmail.com"
            },
            timeout=12
        )
        data = resp.json()
        translated = data.get("responseData", {}).get("translatedText", "")
        status = data.get("responseStatus", 0)

        if status == 200 and translated and "MYMEMORY WARNING" not in translated:
            return translated.strip()
        return ""

    except Exception as e:
        print(f"   âš ï¸ ØªØ±Ø¬Ù…Ù‡ Ù†Ø§Ù…ÙˆÙÙ‚: {e}")
        return ""


# â”€â”€â”€ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(title: str, title_fa: str, summary_fa: str, url: str) -> bool:
    display_title = title_fa if title_fa else title

    lines = [f"ğŸ“° *{display_title}*", ""]

    if summary_fa and len(summary_fa) > 20:
        lines += [summary_fa, ""]

    lines += [
        f"ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø± Ú©Ø§Ù…Ù„]({url})",
        "",
        "_via ASME News_"
    ]

    message = "\n".join(lines)

    try:
        resp = requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            json={
                "chat_id": TELEGRAM_CHAT,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": False,
            },
            timeout=12
        )
        if resp.status_code == 200:
            print(f"   âœ… Ø§Ø±Ø³Ø§Ù„ Ù…ÙˆÙÙ‚")
            return True
        else:
            print(f"   âŒ Ø®Ø·Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù…: {resp.status_code} - {resp.text[:100]}")
            return False
    except Exception as e:
        print(f"   âŒ Ø®Ø·Ø§: {e}")
        return False


# â”€â”€â”€ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_duplicate(databases, url: str) -> bool:
    try:
        r = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            queries=[Query.equal("news_url", url)]
        )
        return r["total"] > 0
    except Exception as e:
        print(f"   âš ï¸ Ø®Ø·Ø§ÛŒ DB Ø¨Ø±Ø±Ø³ÛŒ: {e}")
        return False


def save_to_db(databases, url: str, title: str) -> bool:
    try:
        databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            document_id=ID.unique(),
            data={
                "news_url": url,
                "title": title,
                "published_at": datetime.now(timezone.utc).isoformat()
            }
        )
        return True
    except Exception as e:
        print(f"   âŒ Ø®Ø·Ø§ÛŒ DB Ø°Ø®ÛŒØ±Ù‡: {e}")
        return False


# â”€â”€â”€ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(context):
    print("ğŸš€ Ø´Ø±ÙˆØ¹ ASME News Bot")
    print(f"â° {datetime.now(timezone.utc).isoformat()}")

    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Appwrite
    client = Client()
    client.set_endpoint(APPWRITE_ENDPOINT)
    client.set_project(APPWRITE_PROJECT)
    client.set_key(APPWRITE_KEY)
    databases = Databases(client)

    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±
    news_items = fetch_rss_news()

    if not news_items:
        print("âŒ Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯")
        return context.res.json({"status": "no_news"})

    sent = 0
    skipped = 0

    for item in news_items:
        url   = item["url"]
        title = item["title"]
        desc  = item.get("description", "")

        print(f"\nğŸ“Œ {title[:70]}")

        if is_duplicate(databases, url):
            print(f"   â­ï¸ ØªÚ©Ø±Ø§Ø±ÛŒ")
            skipped += 1
            continue

        # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù†
        title_fa = translate_to_fa(title)
        print(f"   ğŸ”¤ {title_fa[:60] if title_fa else '(ØªØ±Ø¬Ù…Ù‡ Ù†Ø´Ø¯)'}")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ùˆ ØªØ±Ø¬Ù…Ù‡ Ø®Ù„Ø§ØµÙ‡
        article_text = extract_article_text(url, fallback_desc=desc)
        summary_fa = ""
        if article_text and len(article_text) >= 50:
            summary_fa = translate_to_fa(article_text)
            print(f"   ğŸ“ {summary_fa[:60] if summary_fa else '(ØªØ±Ø¬Ù…Ù‡ Ù†Ø´Ø¯)'}")

        # Ø§Ø±Ø³Ø§Ù„
        if send_telegram(title, title_fa, summary_fa, url):
            save_to_db(databases, url, title)
            sent += 1

        time.sleep(1.5)

    result = f"âœ… {sent} Ø§Ø±Ø³Ø§Ù„ | â­ï¸ {skipped} ØªÚ©Ø±Ø§Ø±ÛŒ"
    print(f"\n{result}")
    return context.res.json({"status": "ok", "sent": sent, "skipped": skipped})
