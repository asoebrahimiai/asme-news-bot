import os
import requests
from bs4 import BeautifulSoup
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query
from datetime import datetime, timezone
import time

# Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù…Ø­ØªÙˆØ§
from newspaper import Article, Config

TELEGRAM_TOKEN   = os.environ.get("TELEGRAM_TOKEN", "")
TELEGRAM_CHANNEL = os.environ.get("TELEGRAM_CHANNEL", "")
APPWRITE_ENDPOINT   = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT_ID = os.environ.get("APPWRITE_PROJECT_ID", "")
APPWRITE_API_KEY    = os.environ.get("APPWRITE_API_KEY", "")
DATABASE_ID   = os.environ.get("APPWRITE_DATABASE_ID", "")
COLLECTION_ID = os.environ.get("APPWRITE_COLLECTION_ID", "")

HEADLINES_URL = "https://www.asme.org/about-asme/media-inquiries/asme-in-the-headlines"

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/120.0.0.0 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
}

# â”€â”€â”€ Appwrite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_db():
    client = Client()
    client.set_endpoint(APPWRITE_ENDPOINT)
    client.set_project(APPWRITE_PROJECT_ID)
    client.set_key(APPWRITE_API_KEY)
    return Databases(client)

def is_published(databases, url: str) -> bool:
    try:
        res = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            queries=[Query.equal("news_url", [url])]
        )
        return res["total"] > 0
    except Exception as e:
        print(f"DB check error: {e}")
        return False

def save_to_db(databases, url: str, title: str):
    try:
        databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            document_id=ID.unique(),
            data={
                "news_url": url,
                "title": title,
                "published_at": datetime.now(timezone.utc).isoformat()
            }
        )
    except Exception as e:
        print(f"DB save error: {e}")

# â”€â”€â”€ Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_headlines() -> list:
    print("Fetching headlines...")
    try:
        resp = requests.get(HEADLINES_URL, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except Exception as e:
        print(f"Fetch error: {e}")
        return []

    soup = BeautifulSoup(resp.content, "html.parser")
    news_list = []

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"].strip()
        title = a_tag.get_text(strip=True)

        if not href.startswith("http"):
            continue
        if "asme.org" in href:
            continue
        if len(title) < 20:
            continue
        
        # Ù†Ú©ØªÙ‡: Ø±ÙˆØ´ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…Ù†Ø¨Ø¹ Ù‡Ù…Ú†Ù†Ø§Ù† Ø´Ú©Ù†Ù†Ø¯Ù‡ Ø§Ø³Øª.
        # Ø§ÛŒÙ† Ú©Ø¯ Ø¨Ù‡ Ø¯Ù†Ø¨Ø§Ù„ Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ Ù‡Ù…â€ŒØ³Ø·Ø­ (sibling) Ø¨Ø§ ØªÚ¯ Ù„ÛŒÙ†Ú© Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø¯.
        # Ø§Ú¯Ø± Ø³Ø§Ø®ØªØ§Ø± Ø³Ø§ÛŒØª ASME ØªØºÛŒÛŒØ± Ú©Ù†Ø¯ØŒ Ø§ÛŒÙ† Ø¨Ø®Ø´ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ù†Ø¨Ø¹ Ø±Ø§ Ø§Ø´ØªØ¨Ø§Ù‡ ØªØ´Ø®ÛŒØµ Ø¯Ù‡Ø¯.
        # Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ØŒ Ø¨Ø§ÛŒØ¯ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ù‚ÛŒÙ‚ HTML Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ùˆ Ø³Ù„Ú©ØªÙˆØ± Ø¨Ù‡ØªØ±ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯.
        source = ""
        parent = a_tag.find_parent()
        if parent:
            for sibling in parent.find_all(string=True, recursive=False):
                s = sibling.strip()
                if s and s != title and len(s) > 2:
                    source = s[:80]
                    break

        news_list.append({"url": href, "title": title, "source": source})
        print(f"  Found: {title[:70]}")

    print(f"Total found: {len(news_list)}")
    return news_list[:5]

# â”€â”€â”€ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² ØµÙØ­Ù‡ Ø®Ø¨Ø± (Ù†Ø³Ø®Ù‡ Ø§ØµÙ„Ø§Ø­ Ø´Ø¯Ù‡ Ùˆ Ù‡ÙˆØ´Ù…Ù†Ø¯) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_article_text(url: str) -> str:
    """ÙˆØ±ÙˆØ¯ Ø¨Ù‡ Ù„ÛŒÙ†Ú© Ø®Ø¨Ø± Ùˆ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ø§ØµÙ„ÛŒ Ø¨Ø§ newspaper3k"""
    try:
        # ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ SSL Ùˆ ØªØ¹ÛŒÛŒÙ† Ù‡Ø¯Ø±
        config = Config()
        config.browser_user_agent = HEADERS["User-Agent"]
        config.request_timeout = 20
        config.memoize_articles = False # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ú©Ø´ Ú©Ø±Ø¯Ù† Ø¯Ø± Ù…Ø­ÛŒØ· Ø³Ø±ÙˆØ±Ù„Ø³

        article = Article(url, config=config)
        article.download()
        article.parse()

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ùˆ Ù…Ø­Ø¯ÙˆØ¯ Ú©Ø±Ø¯Ù† Ø¢Ù†
        full_text = article.text
        if not full_text:
            return ""

        # Ú†Ù†Ø¯ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù Ø§ÙˆÙ„ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†ÛŒÙ…
        paragraphs = full_text.split('\n\n')
        summary_text = " ".join(paragraphs[:3])

        # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Û¸Û°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø´Ø¯Ù†
        return summary_text[:800]

    except Exception as e:
        print(f"Article fetch error ({url[:50]}): {e}")
        return ""

# â”€â”€â”€ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ MyMemory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_to_persian(text: str) -> str:
    # Ù†Ú©ØªÙ‡: MyMemory ÛŒÚ© Ø³Ø±ÙˆÛŒØ³ Ø±Ø§ÛŒÚ¯Ø§Ù† Ø¨Ø§ Ù…Ø­Ø¯ÙˆØ¯ÛŒØªâ€ŒÙ‡Ø§ÛŒÛŒ Ø¯Ø± Ú©ÛŒÙÛŒØª Ùˆ ØªØ¹Ø¯Ø§Ø¯ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø§Ø³Øª.
    # Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ®ØµØµÛŒ Ùˆ Ø¯Ù‚ÛŒÙ‚â€ŒØªØ±ØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² APIÙ‡Ø§ÛŒ Ù¾ÙˆÙ„ÛŒ Ù…Ø§Ù†Ù†Ø¯
    # Google Translate API ÛŒØ§ DeepL API Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ù…ÛŒâ€ŒØ´ÙˆØ¯.
    if not text:
        return ""
    try:
        api_url = "https://api.mymemory.translated.net/get"
        params = {"q": text[:900], "langpair": "en|fa"}
        
        resp = requests.get(api_url, params=params, timeout=15)
        resp.raise_for_status()

        data = resp.json()
        result = data.get("responseData", {}).get("translatedText", "")
        
        if result and result.lower() != text.lower():
            return result
        
        # Ø§Ú¯Ø± ØªØ±Ø¬Ù…Ù‡ Ù…ÙˆÙÙ‚ Ù†Ø¨ÙˆØ¯ØŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ù†Ú¯Ø±Ø¯Ø§Ù†
        return ""

    except Exception as e:
        print(f"Translation error: {e}")
        return ""

# â”€â”€â”€ Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(title_fa: str, summary_fa: str, source: str, news_url: str) -> bool:
    msg_parts = [f"ğŸ“° *{title_fa.strip()}*\n"]

    if summary_fa:
        msg_parts.append(f"{summary_fa.strip()}\n")

    if source:
        # Ù¾Ø§Ú©Ø³Ø§Ø²ÛŒ Ù…Ù†Ø¨Ø¹ Ø§Ø² Ú©Ø§Ø±Ø§Ú©ØªØ±Ù‡Ø§ÛŒ Ø§Ø¶Ø§ÙÛŒ
        cleaned_source = source.replace("via ", "").strip()
        msg_parts.append(f"ğŸŒ *Ù…Ù†Ø¨Ø¹:* {cleaned_source}")

    msg_parts.append(f"ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø± Ú©Ø§Ù…Ù„]({news_url})")
    msg_parts.append("\n_via ASME In the Headlines_")

    caption = "\n".join(msg_parts)

    if len(caption) > 4096:
        # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ù¾ÛŒØ§Ù… Ø¯Ø± ØµÙˆØ±Øª Ù†ÛŒØ§Ø²
        summary_cutoff = 4096 - len(title_fa) - len(source) - 200
        summary_fa_short = summary_fa[:summary_cutoff]
        msg_parts = [
            f"ğŸ“° *{title_fa.strip()}*\n",
            f"{summary_fa_short}... (Ø®Ù„Ø§ØµÙ‡ Ø´Ø¯Ù‡)\n",
            f"ğŸŒ *Ù…Ù†Ø¨Ø¹:* {source}",
            f"ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø± Ú©Ø§Ù…Ù„]({news_url})",
            "\n_via ASME In the Headlines_"
        ]
        caption = "\n".join(msg_parts)
    
    api_base = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}"
    try:
        r = requests.post(
            f"{api_base}/sendMessage",
            json={
                "chat_id": TELEGRAM_CHANNEL,
                "text": caption,
                "parse_mode": "Markdown",
                "disable_web_page_preview": False
            },
            timeout=15
        )
        print(f"Telegram status: {r.status_code}")
        if r.status_code != 200:
            print(f"Telegram error body: {r.text[:200]}")
        return r.status_code == 200
    except Exception as e:
        print(f"Telegram exception: {e}")
        return False

# â”€â”€â”€ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(context):
    print("=== ASME Bot Starting ===")
    
    if not all([TELEGRAM_TOKEN, TELEGRAM_CHANNEL, APPWRITE_PROJECT_ID, APPWRITE_API_KEY, DATABASE_ID, COLLECTION_ID]):
        error_msg = "One or more environment variables are not set."
        print(f"Error: {error_msg}")
        return context.res.json({"error": error_msg}, status_code=500)

    databases = get_db()
    news_list = fetch_headlines()

    if not news_list:
        return context.res.json({"published": 0, "message": "No new headlines found"})

    new_count = 0
    log = []

    for news in reversed(news_list): # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§Ø² Ù‚Ø¯ÛŒÙ…ÛŒ Ø¨Ù‡ Ø¬Ø¯ÛŒØ¯
        try:
            if is_published(databases, news["url"]):
                print(f"Skip (already published): {news['url'][:60]}")
                continue

            print(f"\nProcessing: {news['title'][:70]}")

            article_text = extract_article_text(news["url
