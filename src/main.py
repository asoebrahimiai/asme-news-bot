import os
import requests
from bs4 import BeautifulSoup
from datetime import datetime, timezone
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query

# â”€â”€â”€ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù…Ø­ÛŒØ·ÛŒ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN  = os.environ.get("TELEGRAM_TOKEN", "")
TELEGRAM_CHAT   = os.environ.get("TELEGRAM_CHAT", "")
APPWRITE_ENDPOINT = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT  = os.environ.get("APPWRITE_PROJECT", "")
APPWRITE_KEY      = os.environ.get("APPWRITE_KEY", "")
DATABASE_ID     = os.environ.get("DATABASE_ID", "")
COLLECTION_ID   = os.environ.get("COLLECTION_ID", "")

ASME_URL = "https://www.asme.org/about-asme/media-inquiries/asme-in-the-headlines"

HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "en-US,en;q=0.9",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
}

MAX_NEWS = 8  # Ø­Ø¯Ø§Ú©Ø«Ø± ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¯Ø± Ù‡Ø± Ø§Ø¬Ø±Ø§


# â”€â”€â”€ Ø§Ø³Ú©Ø±Ù¾ Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ Ø§Ø² ØµÙØ­Ù‡ ASME â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def scrape_asme_headlines():
    try:
        resp = requests.get(ASME_URL, headers=HEADERS, timeout=20)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "html.parser")

        news_items = []
        seen_urls = set()

        for a in soup.find_all("a", href=True):
            href = a["href"].strip()
            text = a.get_text(strip=True)

            # ÙÛŒÙ„ØªØ± Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø¯Ø§Ø®Ù„ÛŒ ASME Ùˆ ØºÛŒØ±Ù…Ø±ØªØ¨Ø·
            if not href.startswith("http"):
                continue
            if "asme.org" in href:
                continue
            if len(text) < 20:
                continue
            if href in seen_urls:
                continue

            # Ø­Ø°Ù Ú©Ù„Ù…Ø§Øª Ù†Ø§Ø®ÙˆØ§Ø³ØªÙ‡ Ø§Ø² Ø¹Ù†ÙˆØ§Ù†
            skip_words = ["subscribe", "sign in", "log in", "menu", "search", "cookie"]
            if any(w in text.lower() for w in skip_words):
                continue

            seen_urls.add(href)
            news_items.append({"title": text, "url": href})

            if len(news_items) >= MAX_NEWS:
                break

        print(f"âœ… {len(news_items)} Ø®Ø¨Ø± Ø§Ø² ASME ÛŒØ§ÙØª Ø´Ø¯")
        return news_items

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³Ú©Ø±Ù¾ ASME: {e}")
        return []


# â”€â”€â”€ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø§Ø² URL Ø®Ø¨Ø± â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def extract_article_text(url: str) -> str:
    """
    Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø§Ø² URL Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø§Ø² Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ùˆ Ù…Ø­ØªÙˆØ§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ· Ø±Ø§ ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    try:
        resp = requests.get(url, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "html.parser")

        # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ù…ØªÙ†ÛŒ
        for tag in soup(["script", "style", "nav", "footer", "header",
                         "aside", "form", "iframe", "noscript",
                         "figure", "figcaption", "picture",
                         "advertisement", "ads", "sidebar"]):
            tag.decompose()

        # Ø­Ø°Ù Ø§Ù„Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒÛŒ Ø¨Ø§ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§ÛŒ Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
        for tag in soup.find_all(True):
            cls = " ".join(tag.get("class", []))
            if any(w in cls.lower() for w in [
                "ad", "sidebar", "related", "recommend",
                "comment", "social", "share", "promo",
                "newsletter", "subscribe", "popup", "modal"
            ]):
                tag.decompose()

        # Ø³Ù„Ú©ØªÙˆØ±Ù‡Ø§ÛŒ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¯Ø§Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø­ØªÙˆØ§ÛŒ Ø§ØµÙ„ÛŒ
        selectors = [
            "article .content",
            "article .body",
            "article",
            '[class*="article-body"]',
            '[class*="article-content"]',
            '[class*="story-body"]',
            '[class*="post-content"]',
            '[class*="post-body"]',
            '[class*="entry-content"]',
            '[class*="article__body"]',
            '[class*="story__body"]',
            "main article",
            "main .content",
            ".article-text",
            ".story-text",
            ".post-text",
        ]

        text_parts = []

        for selector in selectors:
            el = soup.select_one(selector)
            if not el:
                continue

            paragraphs = el.find_all("p")
            for p in paragraphs:
                t = p.get_text(separator=" ", strip=True)

                # ÙÛŒÙ„ØªØ± Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒ Ú©ÙˆØªØ§Ù‡ ÛŒØ§ Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
                if len(t) < 80:
                    continue

                skip_phrases = [
                    "cookie", "subscribe", "newsletter", "advertisement",
                    "sign up", "log in", "privacy policy", "terms of use",
                    "copyright", "all rights reserved", "follow us",
                    "read more", "click here", "download", "share this",
                    "you might also like", "related articles",
                    # ÙÛŒÙ„ØªØ± Ù…ØªÙ†â€ŒÙ‡Ø§ÛŒ ØªØ§Ø±ÛŒØ®ÛŒ/Ù…Ø°Ù‡Ø¨ÛŒ Ú©Ù‡ Ø±Ø¨Ø·ÛŒ Ø¨Ù‡ Ù…Ù‡Ù†Ø¯Ø³ÛŒ Ù†Ø¯Ø§Ø±Ù†Ø¯
                    "jordan", "gilead", "ephraim", "passover",
                    "biblical", "testament", "scripture",
                ]
                if any(ph in t.lower() for ph in skip_phrases):
                    continue

                text_parts.append(t)

                if len(text_parts) >= 3:  # Ø­Ø¯Ø§Ú©Ø«Ø± Û³ Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ù
                    break

            if text_parts:
                break  # Ø§ÙˆÙ„ÛŒÙ† Ø³Ù„Ú©ØªÙˆØ± Ù…ÙˆÙÙ‚ Ú©Ø§ÙÛŒÙ‡

        # Fallback: Ù‡Ù…Ù‡ p Ù‡Ø§ÛŒ ØµÙØ­Ù‡
        if not text_parts:
            all_p = soup.find_all("p")
            for p in all_p:
                t = p.get_text(separator=" ", strip=True)
                if len(t) > 100:
                    text_parts.append(t)
                if len(text_parts) >= 2:
                    break

        combined = " ".join(text_parts)

        # Ø§Ø¹ØªØ¨Ø§Ø±Ø³Ù†Ø¬ÛŒ Ù†Ù‡Ø§ÛŒÛŒ: Ø§Ú¯Ù‡ Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ØŒ Ø®Ø§Ù„ÛŒ Ø¨Ø±Ú¯Ø±Ø¯ÙˆÙ†
        if len(combined) < 50:
            return ""

        # Ø¨Ø±Ø´ Ø¨Ù‡ ÛµÛ°Û° Ú©Ø§Ø±Ø§Ú©ØªØ± Ø¨Ø±Ø§ÛŒ ØªØ±Ø¬Ù…Ù‡ Ø³Ø±ÛŒØ¹â€ŒØªØ±
        return combined[:500]

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ø§Ø² {url}: {e}")
        return ""


# â”€â”€â”€ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ø§ MyMemory API â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_to_fa(text: str) -> str:
    """
    Ù…ØªÙ† Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ Ø±Ø§ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    Ø§Ø² MyMemory API Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    if not text or not text.strip():
        return ""

    # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² Ø®Ø·Ø§ÛŒ API
    text = text[:480]

    try:
        resp = requests.get(
            "https://api.mymemory.translated.net/get",
            params={
                "q": text,
                "langpair": "en|fa",
                "de": "newsbot@example.com"  # Ø§ÛŒÙ…ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ø§ÙØ²Ø§ÛŒØ´ limit
            },
            timeout=15
        )
        resp.raise_for_status()
        data = resp.json()

        translated = data.get("responseData", {}).get("translatedText", "")
        status = data.get("responseStatus", 0)

        # Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¶Ø¹ÛŒØª ØªØ±Ø¬Ù…Ù‡
        if status == 200 and translated and translated != text:
            # Ø­Ø°Ù Ù¾ÛŒØºØ§Ù… Ø®Ø·Ø§ÛŒ MyMemory
            if "MYMEMORY WARNING" in translated:
                return ""
            return translated.strip()

        return ""

    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}")
        return ""


# â”€â”€â”€ Ø§Ø±Ø³Ø§Ù„ Ù¾ÛŒØ§Ù… Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù… â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(title: str, title_fa: str, summary_fa: str, url: str) -> bool:
    """
    Ù¾ÛŒØ§Ù… Ø®Ø¨Ø± Ø±Ø§ Ø¨Ù‡ Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯.
    """
    # Ø³Ø§Ø®Øª Ù…ØªÙ† Ù¾ÛŒØ§Ù…
    lines = []

    # Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ ÛŒØ§ Ø§Ù†Ú¯Ù„ÛŒØ³ÛŒ
    if title_fa:
        lines.append(f"ğŸ“° *{title_fa}*")
    else:
        lines.append(f"ğŸ“° *{title}*")

    lines.append("")  # Ø®Ø· Ø®Ø§Ù„ÛŒ

    # Ø®Ù„Ø§ØµÙ‡ ÙØ§Ø±Ø³ÛŒ
    if summary_fa:
        lines.append(summary_fa)
        lines.append("")

    # Ù…Ù†Ø¨Ø¹
    lines.append(f"ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø± Ú©Ø§Ù…Ù„]({url})")
    lines.append("")
    lines.append("_via ASME In the Headlines_")

    message = "\n".join(lines)

    try:
        # Ø§Ø±Ø³Ø§Ù„ Ø¨Ø§ sendMessage
        resp = requests.post(
            f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage",
            json={
                "chat_id": TELEGRAM_CHAT,
                "text": message,
                "parse_mode": "Markdown",
                "disable_web_page_preview": False,
            },
            timeout=15
        )

        if resp.status_code == 200:
            print(f"âœ… Ù¾ÛŒØ§Ù… Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title[:50]}...")
            return True
        else:
            print(f"âŒ Ø®Ø·Ø§ÛŒ ØªÙ„Ú¯Ø±Ø§Ù… {resp.status_code}: {resp.text[:200]}")
            return False

    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù…: {e}")
        return False


# â”€â”€â”€ Ø¨Ø±Ø±Ø³ÛŒ ÙˆØ¬ÙˆØ¯ Ø®Ø¨Ø± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def is_duplicate(databases, url: str) -> bool:
    try:
        result = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            queries=[Query.equal("news_url", url)]
        )
        return result["total"] > 0
    except Exception as e:
        print(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return False


# â”€â”€â”€ Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø± Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def save_to_db(databases, url: str, title: str) -> bool:
    try:
        databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            document_id=ID.unique(),
            data={
                "news_url": url,
                "title": title,
                "published_at": datetime.now(timezone.utc).isoformat()
            }
        )
        return True
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return False


# â”€â”€â”€ ØªØ§Ø¨Ø¹ Ø§ØµÙ„ÛŒ Appwrite â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(context):
    print("ğŸš€ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ News Checker Bot")
    print(f"â° Ø²Ù…Ø§Ù†: {datetime.now(timezone.utc).isoformat()}")

    # Ø±Ø§Ù‡â€ŒØ§Ù†Ø¯Ø§Ø²ÛŒ Appwrite
    client = Client()
    client.set_endpoint(APPWRITE_ENDPOINT)
    client.set_project(APPWRITE_PROJECT)
    client.set_key(APPWRITE_KEY)
    databases = Databases(client)

    # Ø§Ø³Ú©Ø±Ù¾ Ø§Ø®Ø¨Ø§Ø±
    news_items = scrape_asme_headlines()
    if not news_items:
        msg = "Ù‡ÛŒÚ† Ø®Ø¨Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."
        print(msg)
        return context.res.json({"status": "no_news", "message": msg})

    sent_count = 0
    skipped_count = 0

    for item in news_items:
        url   = item["url"]
        title = item["title"]

        print(f"\nğŸ“Œ Ù¾Ø±Ø¯Ø§Ø²Ø´: {title[:60]}...")
        print(f"   URL: {url}")

        # Ø¨Ø±Ø±Ø³ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ Ø¨ÙˆØ¯Ù†
        if is_duplicate(databases, url):
            print(f"   â­ï¸ ØªÚ©Ø±Ø§Ø±ÛŒ - Ø±Ø¯ Ø´Ø¯")
            skipped_count += 1
            continue

        # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù†
        title_fa = translate_to_fa(title)
        print(f"   ğŸ”¤ Ø¹Ù†ÙˆØ§Ù† ÙØ§Ø±Ø³ÛŒ: {title_fa[:60] if title_fa else '(ØªØ±Ø¬Ù…Ù‡ Ù†Ø´Ø¯)'}")

        # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡
        article_text = extract_article_text(url)
        print(f"   ğŸ“„ Ù…ØªÙ† Ø§Ø³ØªØ®Ø±Ø§Ø¬ÛŒ: {article_text[:80] if article_text else '(ÛŒØ§ÙØª Ù†Ø´Ø¯)'}...")

        # ØªØ±Ø¬Ù…Ù‡ Ø®Ù„Ø§ØµÙ‡
        summary_fa = ""
        if article_text and len(article_text) >= 50:
            summary_fa = translate_to_fa(article_text)
            print(f"   ğŸ“ Ø®Ù„Ø§ØµÙ‡ ÙØ§Ø±Ø³ÛŒ: {summary_fa[:80] if summary_fa else '(ØªØ±Ø¬Ù…Ù‡ Ù†Ø´Ø¯)'}...")
        else:
            print(f"   âš ï¸ Ù…ØªÙ† Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡ ÛŒØ§ÙØª Ù†Ø´Ø¯")

        # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
        success = send_telegram(title, title_fa, summary_fa, url)

        if success:
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            save_to_db(databases, url, title)
            sent_count += 1
        
        # ØªØ§Ø®ÛŒØ± Ú©ÙˆØªØ§Ù‡ Ø¨ÛŒÙ† Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§
        import time
        time.sleep(1)

    summary = f"âœ… {sent_count} Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯ | â­ï¸ {skipped_count} ØªÚ©Ø±Ø§Ø±ÛŒ Ø±Ø¯ Ø´Ø¯"
    print(f"\n{summary}")

    return context.res.json({
        "status": "ok",
        "sent": sent_count,
        "skipped": skipped_count,
        "message": summary
    })
