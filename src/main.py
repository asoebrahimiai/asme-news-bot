import os
import requests
from bs4 import BeautifulSoup
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query
from datetime import datetime, timezone
import time
from newspaper import Article, Config
import google.generativeai as genai

# â”€â”€â”€ Environment Variables â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ…Ø§Øª ÙØ§Ù†Ú©Ø´Ù† Appwrite Ø³Øª Ø´ÙˆÙ†Ø¯
TELEGRAM_TOKEN    = os.environ.get("TELEGRAM_TOKEN", "")
TELEGRAM_CHANNEL  = os.environ.get("TELEGRAM_CHANNEL", "")
APPWRITE_ENDPOINT   = os.environ.get("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT_ID = os.environ.get("APPWRITE_PROJECT_ID", "")
APPWRITE_API_KEY    = os.environ.get("APPWRITE_API_KEY", "")
DATABASE_ID       = os.environ.get("APPWRITE_DATABASE_ID", "")
COLLECTION_ID     = os.environ.get("APPWRITE_COLLECTION_ID", "")
# Ú©Ù„ÛŒØ¯ API Ø¬Ø¯ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù…Ø¯Ù„ Gemini
GEMINI_API_KEY    = os.environ.get("GEMINI_API_KEY", "")

HEADLINES_URL = "https://www.asme.org/about-asme/media-inquiries/asme-in-the-headlines"

# Ù‡Ø¯Ø± Ø¨Ø±Ø§ÛŒ Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§ Ø¬Ù‡Øª Ø´Ø¨ÛŒÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù…Ø±ÙˆØ±Ú¯Ø±
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
    'Accept-Language': 'en-US,en;q=0.5'
}

# â”€â”€â”€ Appwrite Database Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_db():
    """Ø§ØªØµØ§Ù„ Ø¨Ù‡ Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Appwrite Ø±Ø§ Ø¨Ø±Ù‚Ø±Ø§Ø± Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    client = Client()
    client.set_endpoint(APPWRITE_ENDPOINT)
    client.set_project(APPWRITE_PROJECT_ID)
    client.set_key(APPWRITE_API_KEY)
    return Databases(client)

def is_published(databases, url: str) -> bool:
    """Ø¨Ø±Ø±Ø³ÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ú©Ù‡ Ø¢ÛŒØ§ ÛŒÚ© Ø®Ø¨Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø§Ø³Øª ÛŒØ§ Ø®ÛŒØ±"""
    try:
        res = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            queries=[Query.equal("news_url", [url])]
        )
        return res["total"] > 0
    except Exception as e:
        print(f"Error checking if published in DB: {e}")
        return False

def save_to_db(databases, url: str, title: str):
    """Ø®Ø¨Ø± Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ Ø±Ø§ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³ Ø°Ø®ÛŒØ±Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯ ØªØ§ Ø§Ø² Ø§Ù†ØªØ´Ø§Ø± Ù…Ø¬Ø¯Ø¯ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø´ÙˆØ¯"""
    try:
        databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            document_id=ID.unique(),
            data={
                "news_url": url,
                "title": title,
                "published_at": datetime.now(timezone.utc).isoformat()
            }
        )
    except Exception as e:
        print(f"Error saving to DB: {e}")

# â”€â”€â”€ News Fetching and Parsing â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_headlines() -> list:
    """Ù„ÛŒØ³Øª Ø¹Ù†Ø§ÙˆÛŒÙ† Ø§Ø®Ø¨Ø§Ø± Ø±Ø§ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ ASME Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    print("Fetching headlines from ASME...")
    try:
        resp = requests.get(HEADLINES_URL, headers=HEADERS, timeout=20)
        resp.raise_for_status()
    except requests.RequestException as e:
        print(f"Error fetching headlines page: {e}")
        return []

    soup = BeautifulSoup(resp.content, "html.parser")
    news_list = []

    for a_tag in soup.find_all("a", href=True):
        href = a_tag["href"].strip()
        title = a_tag.get_text(strip=True)

        if not href.startswith("http") or "asme.org" in href or len(title) < 20:
            continue

        source = ""
        parent_tag = a_tag.find_parent()
        if parent_tag:
            # ØªÙ„Ø§Ø´ Ø¨Ø±Ø§ÛŒ Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù†Ø§Ù… Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±
            source_candidate = parent_tag.get_text(strip=True).replace(title, "").strip()
            if source_candidate:
                source = source_candidate

        news_list.append({"url": href, "title": title, "source": source})
        print(f"  Found: {title[:70]}")

    print(f"Total relevant headlines found: {len(news_list)}")
    return news_list[:5] # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ûµ Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ø¯Ø± Ù‡Ø± Ø§Ø¬Ø±Ø§

def extract_article_text(url: str) -> str:
    """Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ newspaper3k Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    print(f"  Extracting article from: {url[:60]}")
    try:
        config = Config()
        config.browser_user_agent = HEADERS['User-Agent']
        
        article = Article(url, config=config)
        article.download()
        article.parse()
        
        return article.text
    except Exception as e:
        print(f"  Error extracting article content: {e}")
        return ""

# â”€â”€â”€ Translation Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def translate_to_persian(text: str) -> str:
    """ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙˆÙ† Ú©ÙˆØªØ§Ù‡ (Ù…Ø§Ù†Ù†Ø¯ Ø¹Ù†Ø§ÙˆÛŒÙ†) Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø³Ø±ÙˆÛŒØ³ MyMemory"""
    if not text:
        return ""
    try:
        params = {'q': text, 'langpair': 'en|fa'}
        resp = requests.get("https://api.mymemory.translated.net/get", params=params, timeout=10)
        resp.raise_for_status()
        data = resp.json()
        translated_text = data.get("responseData", {}).get("translatedText", "")
        return translated_text if translated_text else text
    except requests.RequestException as e:
        print(f"  Translation error (MyMemory): {e}")
        return text # Ø¯Ø± ØµÙˆØ±Øª Ø®Ø·Ø§ØŒ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯

def summarize_and_translate_with_gemini(text: str) -> str:
    """Ù…ØªÙ† Ù…Ù‚Ø§Ù„Ù‡ Ø±Ø§ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Gemini 1.5 Flash Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ ØªØ±Ø¬Ù…Ù‡ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    if not text:
        return ""
    if not GEMINI_API_KEY:
        print("  GEMINI_API_KEY is not set. Skipping summary.")
        return ""
        
    print("  Summarizing and translating with Gemini...")
    try:
        genai.configure(api_key=GEMINI_API_KEY)
        model = genai.GenerativeModel('gemini-1.5-flash')
        
        prompt = f"""
        As a professional news editor, your task is to first summarize the following English news article into one or two concise paragraphs, capturing the most important points. Then, translate this summary into fluent and natural Persian.

        RULES:
        1.  Your final output must ONLY be the Persian translation of the summary.
        2.  Do not include any English text, introductory phrases like "Ø®Ù„Ø§ØµÙ‡:" or any explanations.
        3.  The translation should be professional and engaging for a news channel audience.

        ARTICLE TEXT:
        ---
        {text}
        ---
        """
        
        response = model.generate_content(prompt)
        return response.text.strip()
        
    except Exception as e:
        print(f"  Error with Gemini API: {e}")
        return f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ† Ø¨Ø§ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ: {e}"


# â”€â”€â”€ Telegram Sender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(title_fa: str, summary_fa: str, source: str, news_url: str) -> bool:
    """Ù¾ÛŒØ§Ù… Ù†Ù‡Ø§ÛŒÛŒ Ø±Ø§ ÙØ±Ù…Øª Ú©Ø±Ø¯Ù‡ Ùˆ Ø¨Ù‡ Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù… Ø§Ø±Ø³Ø§Ù„ Ù…ÛŒâ€ŒÚ©Ù†Ø¯"""
    msg_parts = [f"**{title_fa}**\n"]

    if summary_fa:
        msg_parts.append(f"{summary_fa}\n")

    if source:
        msg_parts.append(f"ğŸŒ **Ù…Ù†Ø¨Ø¹:** {source}")

    msg_parts.append(f"ğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø®Ø¨Ø± Ú©Ø§Ù…Ù„]({news_url})")
    msg_parts.append("\n*via ASME In the Headlines*")

    message = "\n".join(msg_parts)
    
    # Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø§Ø² Ø§ÛŒÙ†Ú©Ù‡ Ù¾ÛŒØ§Ù… Ø§Ø² Ø­Ø¯ Ù…Ø¬Ø§Ø² ØªÙ„Ú¯Ø±Ø§Ù… Ø·ÙˆÙ„Ø§Ù†ÛŒâ€ŒØªØ± Ù†Ø¨Ø§Ø´Ø¯
    if len(message) > 4096:
        message = message[:4090] + "..."

    api_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
    payload = {
        "chat_id": TELEGRAM_CHANNEL,
        "text": message,
        "parse_mode": "Markdown",
        "disable_web_page_preview": False
    }

    try:
        resp = requests.post(api_url, json=payload, timeout=20)
        print(f"  Telegram response status: {resp.status_code}")
        if resp.status_code != 200:
            print(f"  Telegram error details: {resp.text}")
        return resp.status_code == 200
    except requests.RequestException as e:
        print(f"  Exception while sending to Telegram: {e}")
        return False

# â”€â”€â”€ Main Function (Appwrite Entrypoint) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(context):
    """Ù†Ù‚Ø·Ù‡ Ø´Ø±ÙˆØ¹ Ø§Ø¬Ø±Ø§ÛŒ ÙØ§Ù†Ú©Ø´Ù†"""
    start_time = time.time()
    print(f"====== ASME Bot Execution Started at {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M:%S UTC')} ======")

    if not all([TELEGRAM_TOKEN, TELEGRAM_CHANNEL, APPWRITE_PROJECT_ID, APPWRITE_API_KEY, DATABASE_ID, COLLECTION_ID]):
        error_msg = "One or more required environment variables are not set."
        print(f"FATAL: {error_msg}")
        return context.res.json({"status": "failed", "error": error_msg}, status_code=500)

    databases = get_db()
    news_list = fetch_headlines()

    if not news_list:
        print("No new headlines found to process.")
        return context.res.json({"published": 0, "message": "No headlines found"})

    published_count = 0
    logs = []

    for news in news_list:
        try:
            if is_published(databases, news["url"]):
                print(f"Skipping (already published): {news['url'][:70]}")
                continue

            print(f"\nProcessing: {news['title'][:80]}")

            # 1. ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† (Ø³Ø±ÛŒØ¹ Ùˆ Ø³Ø§Ø¯Ù‡)
            title_fa = translate_to_persian(news["title"])
            print(f"  Translated Title: {title_fa[:70]}")
            time.sleep(1) # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒÙ† Ø¯Ø±Ø®ÙˆØ§Ø³Øªâ€ŒÙ‡Ø§

            # 2. Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ù…ØªÙ† Ú©Ø§Ù…Ù„ Ù…Ù‚Ø§Ù„Ù‡
            article_text = extract_article_text(news["url"])
            print(f"  Extracted article length: {len(article_text)} chars")

            # 3. Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ùˆ ØªØ±Ø¬Ù…Ù‡ Ø¨Ø§ Gemini
            summary_fa = ""
            if article_text:
                summary_fa = summarize_and_translate_with_gemini(article_text)
                print(f"  Gemini summary length: {len(summary_fa)} chars")
            else:
                print("  No article text to summarize.")
            
            # 4. Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
            is_sent = send_telegram(title_fa, summary_fa, news["source"], news["url"])

            if is_sent:
                save_to_db(databases, news["url"], news["title"])
                published_count += 1
                logs.append(f"SUCCESS: {news['title'][:60]}")
                print(f"  Successfully posted and saved: {news['title'][:70]}")
                time.sleep(3) # ÙØ§ØµÙ„Ù‡ Ø¨ÛŒØ´ØªØ± Ø¨Ø¹Ø¯ Ø§Ø² ÛŒÚ© Ø§Ø±Ø³Ø§Ù„ Ù…ÙˆÙÙ‚
            else:
                logs.append(f"FAIL (Telegram): {news['title'][:60]}")
                print(f"  Failed to post to Telegram: {news['title'][:70]}")

        except Exception as e:
            error_log = f"CRITICAL ERROR processing '{news.get('title', 'N/A')}': {e}"
            print(error_log)
            logs.append(error_log)

    end_time = time.time()
    duration = round(end_time - start_time, 2)
    print(f"\n====== Execution Finished in {duration} seconds. Published: {published_count}/{len(news_list)} ======")
    
    return context.res.json({
        "status": "completed",
        "published_count": published_count,
        "total_found": len(news_list),
        "execution_duration_sec": duration,
        "logs": logs
    })
