import os
import requests
import time
import re
import warnings
import json
from datetime import datetime, timezone
from bs4 import BeautifulSoup
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query
from newspaper import Article, Config

# â”€â”€â”€ ğŸ”‡ Suppress Warnings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
warnings.simplefilter("ignore")
os.environ["PYTHONWARNINGS"] = "ignore"

# â”€â”€â”€ ğŸ”¥ ENV VARIABLES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
TELEGRAM_TOKEN     = os.getenv("TELEGRAM_TOKEN")
TELEGRAM_CHANNEL   = os.getenv("TELEGRAM_CHANNEL")
APPWRITE_ENDPOINT  = os.getenv("APPWRITE_ENDPOINT", "https://cloud.appwrite.io/v1")
APPWRITE_PROJECT_ID= os.getenv("APPWRITE_PROJECT_ID")
APPWRITE_API_KEY   = os.getenv("APPWRITE_API_KEY")
DATABASE_ID        = os.getenv("APPWRITE_DATABASE_ID")
COLLECTION_ID      = os.getenv("APPWRITE_COLLECTION_ID")
GROQ_API_KEY       = os.getenv("GROQ_API_KEY") 

HEADLINES_URL = "https://www.asme.org/about-asme/media-inquiries/asme-in-the-headlines"
HEADERS = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/122.0.0.0 Safari/537.36',
    'Accept-Language': 'en-US,en;q=0.9',
}

# â”€â”€â”€ ğŸ›  Helper Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def full_escape_markdown_v2(text: str) -> str:
    escape_chars = r'_*[]()~`>#+-=|{}.!'
    text = re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)
    return text.strip()

def url_safe_encode(url: str) -> str:
    return requests.utils.quote(url, safe=':/?#[]@!$&\'()*+,;=')

# â”€â”€â”€ ğŸ’¾ Appwrite DB â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def get_db():
    client = Client()
    client.set_endpoint(APPWRITE_ENDPOINT).set_project(APPWRITE_PROJECT_ID).set_key(APPWRITE_API_KEY)
    return Databases(client)

def is_published(databases, url: str, context) -> bool:
    try:
        res = databases.list_documents(DATABASE_ID, COLLECTION_ID, [Query.equal("news_url", [url])])
        return res["total"] > 0
    except Exception as e:
        context.log(f"âš ï¸ DB Read Error: {e}")
        return False

def save_to_db(databases, url: str, title: str, context):
    try:
        databases.create_document(DATABASE_ID, COLLECTION_ID, ID.unique(), {
            "news_url": url,
            "title": title[:255],
            "published_at": datetime.now(timezone.utc).isoformat()
        })
        context.log("âœ… Saved to DB.")
    except Exception as e:
        context.log(f"âŒ DB Save Error: {e}")

# â”€â”€â”€ ğŸ“° News Fetching & Cleaning â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_headlines(context):
    try:
        resp = requests.get(HEADLINES_URL, headers=HEADERS, timeout=15)
        resp.raise_for_status()
        soup = BeautifulSoup(resp.content, "html.parser")
        
        content = soup.find('div', class_='sf_colsIn') or soup.find('body')
        if not content: return []

        news = []
        for a in content.find_all("a", href=True):
            href, title = a["href"], a.get_text(strip=True)
            if href.startswith('/'): href = "https://www.asme.org" + href
            
            # ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø±Ø§ÛŒ Ø­Ø°Ù Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ ØºÛŒØ±Ø®Ø¨Ø±ÛŒ
            bad_words = ['about-asme', 'media-inquiries', 'login', 'contact', 'privacy', 'terms']
            if len(title) > 30 and not any(b in href.lower() for b in bad_words):
                if not any(n['url'] == href for n in news):
                    news.append({"url": href, "title": title, "source": "ASME"})
                    
        return news[:9]
    except Exception as e:
        context.log(f"Error fetching headlines: {e}")
        return []

def extract_article_data(url: str, context) -> tuple[str, str]:
    text = ""
    image_url = ""
    
    # Ù…ØªØ¯ 1: Newspaper3k (Ù…Ø¹Ù…ÙˆÙ„Ø§Ù‹ Ù‡ÙˆØ´Ù…Ù†Ø¯ØªØ± Ø§Ø³Øª)
    try:
        config = Config(fetch_images=True, browser_user_agent=HEADERS['User-Agent'], request_timeout=15)
        article = Article(url, config=config)
        article.download()
        article.parse()
        text = article.text.strip()
        image_url = article.top_image
    except Exception:
        pass

    # Ø§Ú¯Ø± Ù…ØªÙ† Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ ÛŒØ§ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ØŒ Ø¨Ø±Ùˆ Ø³Ø±Ø§Øº Ù…ØªØ¯ 2
    if len(text) < 200:
        context.log("âš ï¸ Newspaper3k yielded short text, trying BeautifulSoup cleaning...")
        try:
            resp = requests.get(url, headers=HEADERS, timeout=15)
            if resp.status_code == 200:
                soup = BeautifulSoup(resp.content, "html.parser")
                
                # Ø­Ø°Ù ØªÚ¯â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø­Ù… (ØªØ¨Ù„ÛŒØºØ§ØªØŒ Ù…Ù†ÙˆÙ‡Ø§ØŒ ÙÙˆØªØ±)
                for script in soup(["script", "style", "nav", "footer", "header", "aside", "form"]):
                    script.decompose()
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¨Ø¯Ù†Ù‡ Ø§ØµÙ„ÛŒ Ù…Ù‚Ø§Ù„Ù‡
                article_body = soup.find('article') or soup.find('main') or soup.find('div', class_='content') or soup.body
                
                if article_body:
                    paragraphs = article_body.find_all('p')
                    # ÙÙ‚Ø· Ù¾Ø§Ø±Ø§Ú¯Ø±Ø§Ùâ€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ù‡Ø³ØªÙ†Ø¯ Ø±Ø§ Ù†Ú¯Ù‡ Ø¯Ø§Ø± (Ø­Ø°Ù ØªÛŒØªØ±Ù‡Ø§ÛŒ ØªØ¨Ù„ÛŒØºØ§ØªÛŒ Ú©ÙˆØªØ§Ù‡)
                    clean_paragraphs = [p.get_text(strip=True) for p in paragraphs if len(p.get_text(strip=True)) > 60]
                    text = "\n".join(clean_paragraphs)
                    
                    if not image_url:
                        og_image = soup.find("meta", property="og:image")
                        if og_image: image_url = og_image.get("content", "")
        except Exception:
            pass

    return text, image_url

# â”€â”€â”€ ğŸ§  Groq AI Logic (Strict JSON) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_with_groq(title: str, text: str, context) -> tuple[str, str]:
    if not GROQ_API_KEY:
        return title, "Ú©Ù„ÛŒØ¯ GROQ_API_KEY ØªÙ†Ø¸ÛŒÙ… Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª."

    # Ù¾Ø±Ø§Ù…Ù¾Øª Ø¨Ø³ÛŒØ§Ø± Ø³Ø®Øªâ€ŒÚ¯ÛŒØ±Ø§Ù†Ù‡ Ø¨Ø±Ø§ÛŒ Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªÙˆÙ‡Ù… Ùˆ ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙˆÙ† Ù†Ø§Ù…Ø±Ø¨ÙˆØ·
    prompt = f"""You are a professional engineering news editor.
    
    Task 1: Read the text below. Ignore any "Recommended for you", "Related stories", or advertisements at the end. Focus ONLY on the main story related to the title.
    Task 2: Translate the title to Persian.
    Task 3: Summarize the MAIN story in Persian (2 paragraphs). Do not include unrelated topics like 'dog rescue' or 'scandals' unless they are the main topic.

    Source Title: {title}
    Source Text: {text[:3500]}

    Output JSON Format:
    {{
      "title_fa": "Persian Title",
      "summary_fa": "Persian Summary"
    }}"""

    url = "https://api.groq.com/openai/v1/chat/completions"
    headers = {
        "Authorization": f"Bearer {GROQ_API_KEY}",
        "Content-Type": "application/json"
    }
    
    payload = {
        "model": "llama-3.3-70b-versatile",
        "messages": [
            {"role": "system", "content": "You are a JSON-only response bot. You filter out junk text."},
            {"role": "user", "content": prompt}
        ],
        "temperature": 0.2, # Ú©Ø§Ù‡Ø´ Ø®Ù„Ø§Ù‚ÛŒØª Ø¨Ø±Ø§ÛŒ Ø¯Ù‚Øª Ø¨ÛŒØ´ØªØ±
        "response_format": {"type": "json_object"}
    }
    
    try:
        resp = requests.post(url, headers=headers, json=payload, timeout=30)
        
        if resp.status_code == 200:
            data = resp.json()
            content_str = data['choices'][0]['message']['content']
            parsed = json.loads(content_str)
            return parsed.get("title_fa", title), parsed.get("summary_fa", "Ø®Ù„Ø§ØµÙ‡ ØªÙˆÙ„ÛŒØ¯ Ù†Ø´Ø¯.")
        else:
            context.log(f"âš ï¸ Groq Error: {resp.status_code} - {resp.text}")
            # Ø§Ú¯Ø± Ø®Ø·Ø§ÛŒ Û´Û²Û¹ ÛŒØ§ ÛµÛ°Û° Ø¯Ø§Ø¯ØŒ Ø§Ø±ÙˆØ± ÙˆØ§Ù‚Ø¹ÛŒ Ú¯Ø±ÙˆÙ‚ Ø±Ø§ Ø¨Ø±Ù…ÛŒâ€ŒÚ¯Ø±Ø¯Ø§Ù†Ø¯ Ù†Ù‡ Ø§Ø±ÙˆØ± Ú¯ÙˆÚ¯Ù„ Ø±Ø§
            return title, f"Ø®Ø·Ø§ÛŒ Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Groq (Ú©Ø¯ {resp.status_code})"
            
    except Exception as e:
        context.log(f"ğŸ’¥ Groq Exception: {e}")
        return title, "Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Ø³Ø±ÙˆØ± Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ."

# â”€â”€â”€ âœˆï¸ Telegram Sender â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def send_telegram(title_fa: str, summary_fa: str, source: str, url: str, image_url: str, context) -> bool:
    safe_title = full_escape_markdown_v2(title_fa)
    safe_source = full_escape_markdown_v2(source)
    safe_url = url_safe_encode(url)
    
    # Ú©ÙˆØªØ§Ù‡ Ú©Ø±Ø¯Ù† Ø®Ù„Ø§ØµÙ‡ Ø§Ú¯Ø± Ø®ÛŒÙ„ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ Ø¨ÙˆØ¯ (Ø¨Ø±Ø§ÛŒ Ú©Ù¾Ø´Ù† Ø¹Ú©Ø³)
    if len(summary_fa) > 850: summary_fa = summary_fa[:850] + "..."
    safe_summary = full_escape_markdown_v2(summary_fa)

    caption = f"*{safe_title}*\n\n{safe_summary}\n\nğŸŒ Ù…Ù†Ø¨Ø¹: {safe_source}\nğŸ”— [Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ú©Ø§Ù…Ù„]({safe_url})"
    
    # Ù…ØªØ¯ Ø§Ø±Ø³Ø§Ù„ Ø¹Ú©Ø³
    if image_url and image_url.startswith('http'):
        api_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendPhoto"
        payload = {
            "chat_id": TELEGRAM_CHANNEL, 
            "photo": image_url, 
            "caption": caption, 
            "parse_mode": "MarkdownV2"
        }
    else:
        # Ù…ØªØ¯ Ø§Ø±Ø³Ø§Ù„ Ù…ØªÙ† (Ø§Ú¯Ø± Ø¹Ú©Ø³ Ù†Ø¨ÙˆØ¯)
        api_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
        payload = {
            "chat_id": TELEGRAM_CHANNEL, 
            "text": caption, 
            "parse_mode": "MarkdownV2",
            "disable_web_page_preview": False
        }

    try:
        resp = requests.post(api_url, json=payload, timeout=20)
        if resp.status_code == 200:
            context.log("âœ… Telegram sent.")
            return True
        else:
            context.log(f"âŒ TG Error {resp.status_code}: {resp.text}")
            # Ø§Ú¯Ø± Ø§Ø±Ø³Ø§Ù„ Ø¹Ú©Ø³ Ø´Ú©Ø³Øª Ø®ÙˆØ±Ø¯ØŒ Ù…ØªÙ† Ø®Ø§Ù„ÛŒ Ø¨ÙØ±Ø³Øª
            if "photo" in payload:
                context.log("ğŸ”„ Retrying as text...")
                payload.pop("photo")
                payload.pop("caption")
                payload["text"] = caption
                api_url = f"https://api.telegram.org/bot{TELEGRAM_TOKEN}/sendMessage"
                requests.post(api_url, json=payload, timeout=20)
                return True
    except Exception as e:
        context.log(f"ğŸ’¥ TG Network Error: {e}")
    
    return False

# â”€â”€â”€ ğŸ Main Execution â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def main(context):
    start_time = time.time()
    context.log("ğŸš€ NewsBot v13.0 - CLEAN & PURE GROQ")

    if not all([TELEGRAM_TOKEN, TELEGRAM_CHANNEL, GROQ_API_KEY]):
        context.log("âŒ CRITICAL: Missing ENV Variables")
        return context.res.json({"error": "Missing ENV"})

    db = get_db()
    headlines = fetch_headlines(context)
    
    success_count = 0
    for item in headlines:
        if time.time() - start_time > 110: break # Ø¬Ù„ÙˆÚ¯ÛŒØ±ÛŒ Ø§Ø² ØªØ§ÛŒÙ…â€ŒØ§ÙˆØª

        if is_published(db, item['url'], context):
            context.log(f"â­ï¸ Skipping (Exists): {item['title'][:20]}...")
            continue

        context.log(f"ğŸ”„ Processing: {item['title'][:30]}...")
        text, image_url = extract_article_data(item['url'], context)
        
        # Ø§Ú¯Ø± Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ù¾ÛŒØ¯Ø§ Ù†Ø´Ø¯ ÛŒØ§ Ø®ÛŒÙ„ÛŒ Ú©ÙˆØªØ§Ù‡ Ø¨ÙˆØ¯ (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ ØµÙØ­Ù‡ Ù„Ø§Ú¯ÛŒÙ† ÛŒØ§ ÙˆÛŒØ¯ÛŒÙˆ)
        if len(text) < 150:
            context.log("âš ï¸ Text too short/irrelevant. Skipping.")
            continue

        title_fa, summary_fa = summarize_with_groq(item['title'], text, context)

        # Ø§Ú¯Ø± Ø®Ø±ÙˆØ¬ÛŒ Ù‡Ù†ÙˆØ² Ø­Ø§ÙˆÛŒ Ø§Ø±ÙˆØ± Ù‚Ø¯ÛŒÙ…ÛŒ Ú¯ÙˆÚ¯Ù„ Ø¨ÙˆØ¯ (Ù…Ø­Ø¶ Ø§Ø­ØªÛŒØ§Ø·)
        if "Ù…Ø­Ø¯ÙˆØ¯ÛŒØª Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø±Ø§ÛŒÚ¯Ø§Ù†" in summary_fa:
            summary_fa = "Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…ØªÙ†."

        if send_telegram(title_fa, summary_fa, item['source'], item['url'], image_url, context):
            save_to_db(db, item['url'], item['title'], context)
            success_count += 1
            time.sleep(2)

    return context.res.json({"ok": True, "sent": success_count})
