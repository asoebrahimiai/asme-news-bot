import os
import time
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from appwrite.client import Client
from appwrite.services.databases import Databases
from appwrite.id import ID
from appwrite.query import Query
import telegram
import asyncio
from googletrans import Translator
from datetime import datetime

# Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ù…Ø­ÛŒØ·ÛŒ
load_dotenv()

# ==================== ØªÙ†Ø¸ÛŒÙ…Ø§Øª ====================
TELEGRAM_TOKEN = os.getenv("TELEGRAM_TOKEN")
TELEGRAM_CHANNEL = os.getenv("TELEGRAM_CHANNEL")
APPWRITE_ENDPOINT = os.getenv("APPWRITE_ENDPOINT")
APPWRITE_PROJECT_ID = os.getenv("APPWRITE_PROJECT_ID")
APPWRITE_API_KEY = os.getenv("APPWRITE_API_KEY")
DATABASE_ID = os.getenv("APPWRITE_DATABASE_ID")
COLLECTION_ID = os.getenv("APPWRITE_COLLECTION_ID")

ASME_URL = "https://www.asme.org/topics-resources/society-news/asme-news"

# ==================== Ø§ØªØµØ§Ù„ Ø¨Ù‡ Appwrite ====================
client = Client()
client.set_endpoint(APPWRITE_ENDPOINT)
client.set_project(APPWRITE_PROJECT_ID)
client.set_key(APPWRITE_API_KEY)
databases = Databases(client)

# ==================== Ù…ØªØ±Ø¬Ù… ====================
translator = Translator()


def translate_to_persian(text):
    """ØªØ±Ø¬Ù…Ù‡ Ù…ØªÙ† Ø¨Ù‡ ÙØ§Ø±Ø³ÛŒ"""
    try:
        result = translator.translate(text, dest='fa')
        return result.text
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± ØªØ±Ø¬Ù…Ù‡: {e}")
        return text


def is_news_published(news_url):
    """Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†Ø¯ Ø¢ÛŒØ§ Ø§ÛŒÙ† Ø®Ø¨Ø± Ù‚Ø¨Ù„Ø§Ù‹ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ØŸ"""
    try:
        result = databases.list_documents(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            queries=[Query.equal("news_url", news_url)]
        )
        return result['total'] > 0
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ø¯ÛŒØªØ§Ø¨ÛŒØ³: {e}")
        return False


def save_news_to_db(news_url, title):
    """Ø°Ø®ÛŒØ±Ù‡ Ø®Ø¨Ø± Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³"""
    try:
        databases.create_document(
            database_id=DATABASE_ID,
            collection_id=COLLECTION_ID,
            document_id=ID.unique(),
            data={
                "news_url": news_url,
                "title": title,
                "published_at": datetime.utcnow().isoformat()
            }
        )
        print(f"Ø®Ø¨Ø± Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯: {title}")
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø°Ø®ÛŒØ±Ù‡: {e}")


def scrape_asme_news():
    """Ø¯Ø±ÛŒØ§ÙØª Ù„ÛŒØ³Øª Ø§Ø®Ø¨Ø§Ø± Ø§Ø² Ø³Ø§ÛŒØª ASME"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        response = requests.get(ASME_URL, headers=headers, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        news_list = []
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù…Ø·Ø§Ù„Ø¨ Ø®Ø¨Ø±ÛŒ
        articles = soup.find_all('article') or soup.find_all(class_=['card', 'news-item', 'article-card'])
        
        if not articles:
            # Ø±ÙˆØ´ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† - Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©â€ŒÙ‡Ø§ÛŒ Ø®Ø¨Ø±ÛŒ
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href', '')
                if '/topics-resources/content/' in href or '/topics-resources/society-news/' in href:
                    full_url = href if href.startswith('http') else f"https://www.asme.org{href}"
                    title = link.get_text(strip=True)
                    if title and len(title) > 20:
                        news_list.append({
                            'url': full_url,
                            'title': title,
                            'image': None
                        })
        
        for article in articles[:10]:  # ÙÙ‚Ø· Û±Û° Ø®Ø¨Ø± Ø¢Ø®Ø±
            try:
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„ÛŒÙ†Ú©
                link = article.find('a', href=True)
                if not link:
                    continue
                    
                href = link.get('href', '')
                full_url = href if href.startswith('http') else f"https://www.asme.org{href}"
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ù†ÙˆØ§Ù†
                title_tag = article.find(['h2', 'h3', 'h4'])
                title = title_tag.get_text(strip=True) if title_tag else link.get_text(strip=True)
                
                # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³
                img_tag = article.find('img')
                image_url = None
                if img_tag:
                    image_url = img_tag.get('src') or img_tag.get('data-src')
                    if image_url and not image_url.startswith('http'):
                        image_url = f"https://www.asme.org{image_url}"
                
                if title and full_url:
                    news_list.append({
                        'url': full_url,
                        'title': title,
                        'image': image_url
                    })
                    
            except Exception as e:
                print(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù‚Ø§Ù„Ù‡: {e}")
                continue
        
        return news_list
        
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø§ÛŒØª: {e}")
        return []


def get_article_details(url):
    """Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª ÛŒÚ© Ø®Ø¨Ø± Ø®Ø§Øµ"""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
    }
    try:
        response = requests.get(url, headers=headers, timeout=30)
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø®Ù„Ø§ØµÙ‡ Ø®Ø¨Ø±
        description = ""
        meta_desc = soup.find('meta', {'name': 'description'})
        if meta_desc:
            description = meta_desc.get('content', '')
        
        if not description:
            paragraphs = soup.find_all('p')
            for p in paragraphs[:3]:
                text = p.get_text(strip=True)
                if len(text) > 50:
                    description += text + " "
                    if len(description) > 300:
                        break
        
        # Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ø¹Ú©Ø³ Ø§ØµÙ„ÛŒ
        image_url = None
        og_image = soup.find('meta', {'property': 'og:image'})
        if og_image:
            image_url = og_image.get('content')
        
        if not image_url:
            img = soup.find('img', class_=['hero', 'featured', 'main-image'])
            if img:
                image_url = img.get('src')
                if image_url and not image_url.startswith('http'):
                    image_url = f"https://www.asme.org{image_url}"
        
        return description[:500], image_url
        
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª: {e}")
        return "", None


async def send_to_telegram(title_fa, description_fa, image_url, source_url):
    """Ø§Ø±Ø³Ø§Ù„ Ø®Ø¨Ø± Ø¨Ù‡ Ú©Ø§Ù†Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù…"""
    bot = telegram.Bot(token=TELEGRAM_TOKEN)
    
    # Ù…ØªÙ† Ù¾ÛŒØ§Ù…
    message = f"""ğŸ“° *{title_fa}*

{description_fa}

ğŸ”— [Ù…Ù†Ø¨Ø¹ Ø®Ø¨Ø±]({source_url})
ğŸŒ ASME News
"""
    
    try:
        if image_url:
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ø§ Ø¹Ú©Ø³
            await bot.send_photo(
                chat_id=TELEGRAM_CHANNEL,
                photo=image_url,
                caption=message,
                parse_mode='Markdown'
            )
        else:
            # Ø§Ø±Ø³Ø§Ù„ Ø¨Ø¯ÙˆÙ† Ø¹Ú©Ø³
            await bot.send_message(
                chat_id=TELEGRAM_CHANNEL,
                text=message,
                parse_mode='Markdown',
                disable_web_page_preview=False
            )
        print(f"Ø®Ø¨Ø± Ø§Ø±Ø³Ø§Ù„ Ø´Ø¯: {title_fa[:50]}...")
        return True
    except Exception as e:
        print(f"Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±Ø³Ø§Ù„ ØªÙ„Ú¯Ø±Ø§Ù…: {e}")
        return False


async def main_process():
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø§ØµÙ„ÛŒ"""
    print(f"\n{'='*50}")
    print(f"Ø´Ø±ÙˆØ¹ Ø¨Ø±Ø±Ø³ÛŒ: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print('='*50)
    
    # Ø¯Ø±ÛŒØ§ÙØª Ø§Ø®Ø¨Ø§Ø±
    news_list = scrape_asme_news()
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± ÛŒØ§ÙØª Ø´Ø¯Ù‡: {len(news_list)}")
    
    new_count = 0
    
    for news in news_list:
        url = news['url']
        title = news['title']
        
        # Ú†Ú© Ù…ÛŒâ€ŒÚ©Ù†ÛŒÙ… Ø¢ÛŒØ§ Ù‚Ø¨Ù„Ø§Ù‹ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡ØŸ
        if is_news_published(url):
            print(f"Ù‚Ø¨Ù„Ø§Ù‹ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡: {title[:50]}")
            continue
        
        print(f"Ø®Ø¨Ø± Ø¬Ø¯ÛŒØ¯ Ù¾ÛŒØ¯Ø§ Ø´Ø¯: {title[:50]}")
        
        # Ø¯Ø±ÛŒØ§ÙØª Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±
        description, image_url = get_article_details(url)
        
        # Ø§Ú¯Ø± Ø¹Ú©Ø³ Ø§Ø² ØµÙØ­Ù‡ Ø§ØµÙ„ÛŒ Ø¯Ø§Ø±ÛŒÙ…ØŒ Ø¢Ù† Ø±Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†
        if news['image'] and not image_url:
            image_url = news['image']
        
        # ØªØ±Ø¬Ù…Ù‡ Ø¹Ù†ÙˆØ§Ù† Ùˆ ØªÙˆØ¶ÛŒØ­
        print("Ø¯Ø± Ø­Ø§Ù„ ØªØ±Ø¬Ù…Ù‡...")
        title_fa = translate_to_persian(title)
        description_fa = translate_to_persian(description) if description else ""
        
        # Ø§Ø±Ø³Ø§Ù„ Ø¨Ù‡ ØªÙ„Ú¯Ø±Ø§Ù…
        success = await send_to_telegram(title_fa, description_fa, image_url, url)
        
        if success:
            # Ø°Ø®ÛŒØ±Ù‡ Ø¯Ø± Ø¯ÛŒØªØ§Ø¨ÛŒØ³
            save_news_to_db(url, title)
            new_count += 1
            
            # ØµØ¨Ø± Ú©Ù† ØªØ§ spam Ù†Ø´ÛŒ
            await asyncio.sleep(3)
    
    print(f"\nØªØ¹Ø¯Ø§Ø¯ Ø§Ø®Ø¨Ø§Ø± Ø¬Ø¯ÛŒØ¯ Ù…Ù†ØªØ´Ø± Ø´Ø¯Ù‡: {new_count}")
    print("Ø¨Ø±Ø±Ø³ÛŒ ØªÙ…Ø§Ù… Ø´Ø¯.")


def run():
    """Ø§Ø¬Ø±Ø§ÛŒ Ø§ØµÙ„ÛŒ"""
    while True:
        asyncio.run(main_process())
        
        # ØµØ¨Ø± Û± Ø³Ø§Ø¹Øª
        print(f"\nØµØ¨Ø± Û± Ø³Ø§Ø¹Øª ØªØ§ Ø¨Ø±Ø±Ø³ÛŒ Ø¨Ø¹Ø¯ÛŒ...")
        time.sleep(3600)


if __name__ == "__main__":
    run()
